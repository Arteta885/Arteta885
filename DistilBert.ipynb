{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d9c7fb-6925-4795-8939-d9cf04d238c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis pipeline...\n",
      "Processing file: 2001_news1.pdf\n",
      "Processing file: 2001_news2.pdf\n",
      "Processing file: 2003_message_1.pdf\n",
      "Processing file: 2003_news1.pdf\n",
      "Processing file: 2004_news1.pdf\n",
      "Processing file: 2004_news2.pdf\n",
      "Processing file: 2005_news1.pdf\n",
      "Processing file: 2010_news1.pdf\n",
      "Processing file: 2011_news1.pdf\n",
      "Processing file: 2011_news2.pdf\n",
      "Processing file: 2011_news3.pdf\n",
      "Processing file: 2013_news1.pdf\n",
      "Processing file: 2013_news10.pdf\n",
      "Processing file: 2013_news11.pdf\n",
      "Processing file: 2013_news2.pdf\n",
      "Processing file: 2013_news3.pdf\n",
      "Processing file: 2013_news4.pdf\n",
      "Processing file: 2013_news5.pdf\n",
      "Processing file: 2013_news6.pdf\n",
      "Processing file: 2013_news7.pdf\n",
      "Processing file: 2013_news8.pdf\n",
      "Processing file: 2013_news9.pdf\n",
      "Processing file: 2013_statement1.pdf\n",
      "Processing file: 2018_news1.pdf\n",
      "Processing file: 2018_news2.pdf\n",
      "Processing file: 2018_news3.pdf\n",
      "Processing file: 2018_news4.pdf\n",
      "Processing file: 2018_news5.pdf\n",
      "Processing file: 2018_news6.pdf\n",
      "Processing file: 2018_news7.pdf\n",
      "Processing file: 2018_statement1.pdf\n",
      "Processing file: 2020_message1.pdf\n",
      "Processing file: 2020_news1.pdf\n",
      "Processing file: 2020_news2.pdf\n",
      "Processing file: 2020_news3.pdf\n",
      "Processing file: 2020_news4.pdf\n",
      "Processing file: 2020_news5.pdf\n",
      "Processing file: 2020_statement1.pdf\n",
      "Processing file: 43f313210 2003.pdf\n",
      "Processing file: 45377c2dc.pdf\n",
      "Processing file: A_59_316-EN.pdf\n",
      "Processing file: A_60_306-EN.pdf\n",
      "Processing file: A_60_PV.64-EN.pdf\n",
      "Processing file: A_62_264-EN.pdf\n",
      "Processing file: A_62_318-EN.pdf\n",
      "Processing file: A_63_322-EN.pdf\n",
      "Processing file: A_65_364-EN.pdf\n",
      "Processing file: A_65_391-EN.pdf\n",
      "Processing file: A_66_322-EN.pdf\n",
      "Processing file: A_66_343-EN.pdf\n",
      "Processing file: A_68_319-EN.pdf\n",
      "Processing file: A_68_392-EN.pdf\n",
      "Processing file: A_73_308-EN.pdf\n",
      "Processing file: A_73_386-EN.pdf\n",
      "Processing file: A_73_L-60-EN.pdf\n",
      "Processing file: A_75_271-EN.pdf\n",
      "Processing file: A_75_348-EN.pdf\n",
      "Processing file: A_75_388-EN.pdf\n",
      "Processing file: A_C.3_60_L.48-EN.pdf\n",
      "Processing file: A_C.3_60_SR.42-EN.pdf\n",
      "Processing file: A_C.3_62_L.37-EN.pdf\n",
      "Processing file: A_C.3_62_L.37_Rev.1-EN.pdf\n",
      "Processing file: A_C.3_63_L.26-EN2008년.pdf\n",
      "Processing file: A_C.3_65_L.47-EN.pdf\n",
      "Processing file: A_C.3_66_L.54-EN 2011.pdf\n",
      "Processing file: A_C.3_68_L.56-EN.pdf\n",
      "Processing file: A_C.3_73_L.40-EN.pdf\n",
      "Processing file: A_C.3_75_L.30-EN.pdf\n",
      "Processing file: A_HRC_13_13-EN.pdf\n",
      "Processing file: A_HRC_13_47-EN.pdf\n",
      "Processing file: A_HRC_13_L.13-EN.pdf\n",
      "Processing file: A_HRC_16_58-EN.pdf\n",
      "Processing file: A_HRC_16_L.3-EN.pdf\n",
      "Processing file: A_HRC_22_57-EN.pdf\n",
      "Processing file: A_HRC_22_L.19-EN.pdf\n",
      "Processing file: A_HRC_22_NGO_126-EN.pdf\n",
      "Processing file: A_HRC_22_NGO_56-EN.pdf\n",
      "Processing file: A_HRC_25_62-EN.pdf\n",
      "Processing file: A_HRC_37_56_Add-3-EN.pdf\n",
      "Processing file: A_HRC_37_69-EN.pdf\n",
      "Processing file: A_HRC_37_L.29-EN.pdf\n",
      "Processing file: A_HRC_43_58-EN.pdf\n",
      "Processing file: A_HRC_43_L.17-EN.pdf\n",
      "Processing file: A_HRC_4_15-EN.pdf\n",
      "Processing file: A_HRC_4_60-EN.pdf\n",
      "Processing file: A_HRC_4_NGO_151-EN.pdf\n",
      "Processing file: A_HRC_7_20-EN.pdf\n",
      "Processing file: A_HRC_7_47-EN.pdf\n",
      "Processing file: A_HRC_7_L.28-EN.pdf\n",
      "Processing file: A_HRC_DEC_13_112-EN.pdf\n",
      "Processing file: A_HRC_RES_13_14-EN.pdf\n",
      "Processing file: A_HRC_RES_16_8-EN.pdf\n",
      "Processing file: A_HRC_RES_37_28-EN.pdf\n",
      "Processing file: A_HRC_RES_43_25-EN.pdf\n",
      "Processing file: A_HRC_RES_7_15.pdf\n",
      "Processing file: A_RES_60_173-EN 2005.pdf\n",
      "Processing file: A_RES_60_173-EN.pdf\n",
      "Processing file: A_RES_62_167-EN여기서부터2007.pdf\n",
      "Processing file: A_RES_65_225-EN2010년부터.pdf\n",
      "Processing file: A_RES_68_183-EN2013년.pdf\n",
      "Processing file: A_RES_72_188-EN.pdf\n",
      "Processing file: A_RES_73_180-EN 여기서부터2018.pdf\n",
      "Processing file: A_RES_75_190-EN_여기서부터2020.pdf\n",
      "Processing file: E-CN_4-RES-2004-13.pdf\n",
      "Processing file: E-CN_4-SUB_2-RES-1997-3.1997.pdf\n",
      "Processing file: E-CN_4-SUB_2-RES-1998-2.1998.pdf\n",
      "Processing file: E_CN-4_2005_34-EN.pdf\n",
      "Processing file: E_CN-4_Sub-2_1998_L-7-EN.pdf\n",
      "Processing file: E_CN.4_2003_L.31-EN.pdf\n",
      "Processing file: E_CN.4_2003_L.31_Rev.1-EN.pdf\n",
      "Processing file: E_CN.4_2004_31-EN (1).pdf\n",
      "Processing file: E_CN.4_2004_L.21-EN.pdf\n",
      "Processing file: E_CN.4_2004_NGO_166-EN.pdf\n",
      "Processing file: E_CN.4_2005_32-EN 2004.pdf\n",
      "Processing file: E_CN.4_2005_L.30-EN.pdf\n",
      "Processing file: E_CN.4_2005_NGO_213-EN.pdf\n",
      "Processing file: E_CN.4_2005_NGO_227-EN.pdf\n",
      "Processing file: E_CN.4_Sub.2_1997_L.13-EN.pdf\n",
      "Processing file: HRI_CORE_1_Add.108-EN.2000.pdf\n",
      "Processing file: HRI_CORE_1_Add.108_Rev.1-EN 2002.pdf\n",
      "Preprocessing text...\n",
      "Text preprocessing completed.\n",
      "Starting sentiment analysis...\n",
      "# 1: split\n",
      "Starting sentence splitting...\n",
      "Sentence splitting completed.\n",
      "# 2: filtering\n",
      "# 3: sentiment analysis\n",
      "# 4: result\n",
      "Sentiment analysis completed.\n",
      "Sentiment analysis pipeline completed.\n",
      "Positive: 9343, Average Score: 0.92\n",
      "Negative: 9879, Average Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import PyPDF2\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# 사전 학습된 감성 분석 BERT 모델과 토크나이저 로드\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 장치 설정\n",
    "if torch.cuda.is_available():\n",
    "    device = 0\n",
    "    model.to('cuda')  # 모델을 GPU로 이동\n",
    "else:\n",
    "    print(\"cuda is NOT available\")\n",
    "    sys.exit()\n",
    "\n",
    "# 감정 분석 파이프라인 생성 (GPU 사용 설정)\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# 폴더 경로 설정\n",
    "pdf_folder = r'C:/Users/dlsdn/Desktop/paper/peace'\n",
    "\n",
    "# PDF에서 텍스트 추출 함수\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {filename}\")  # 처리 중인 파일 출력\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page_number, page in enumerate(reader.pages):\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                    text += page_text\n",
    "    return text\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    print(\"Preprocessing text...\")\n",
    "    \n",
    "    # 모든 대문자를 소문자로 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 숫자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 'leader'와 'leadership'을 'kim'으로 변경\n",
    "    text = text.replace('leader', 'kim').replace('leadership', 'kim')\n",
    "    \n",
    "    # 특수문자 제거 (쉼표와 마침표 제외)\n",
    "    text = re.sub(r'[^\\w\\s.,]', '', text)\n",
    "    \n",
    "    # 월 이름 제거\n",
    "    months = [\n",
    "        'january', 'february', 'march', 'april', 'may', 'june',\n",
    "        'july', 'august', 'september', 'october', 'november', 'december'\n",
    "    ]\n",
    "    text = re.sub(r'\\b(?:' + '|'.join(months) + r')\\b', '', text)\n",
    "    \n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    print(\"Text preprocessing completed.\")\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# 사용자 정의 문장 분리 함수\n",
    "def custom_sentence_split(text):\n",
    "    print(\"Starting sentence splitting...\")\n",
    "    # 쉼표와 마침표를 기준으로 문장 나누기\n",
    "    sentences = re.split(r'(?<=\\.)\\s+|(?<=\\,)\\s+', text)\n",
    "\n",
    "    # 빈 문자열 제거\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    print(\"Sentence splitting completed.\")\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# 문장 단위로 분리하여 감정 분석 수행 함수\n",
    "def analyze_sentiment_text(text):\n",
    "    print(\"Starting sentiment analysis...\")\n",
    "\n",
    "    print(\"# 1: split\")\n",
    "    # 1. 텍스트를 문장 단위로 나누기\n",
    "    sentences = custom_sentence_split(text)\n",
    "\n",
    "    print(\"# 2: filtering\")\n",
    "    # 2. 문장 길이 필터링: 문장의 단어 수가 5개 이상 512개 이하인 문장만 선택\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        if 5 <= len(tokens) <= 512:\n",
    "            filtered_sentences.append(sentence)\n",
    "\n",
    "    print(\"# 3: sentiment analysis\")\n",
    "    # 3. 각 문장에 대해 감정 분석 수행\n",
    "    results = []\n",
    "    \n",
    "    for sentence in filtered_sentences:\n",
    "        # 문장이 너무 긴 경우 최대 길이에 맞게 잘라내기\n",
    "        if len(tokenizer.encode(sentence, truncation=True)) > 512:\n",
    "            continue  # 이 문장은 건너뛰고 다음 문장으로 넘어갑니다.\n",
    "\n",
    "        result = sentiment_analysis(sentence)\n",
    "        results.append(result[0])  # 결과는 리스트에 포함되므로 첫 번째 요소를 추가\n",
    "\n",
    "    print(\"# 4: result\")\n",
    "    # 4. 결과 집계 (여기서는 간단히 레이블 빈도수 세기)\n",
    "    label_count = Counter()\n",
    "    score_sum = Counter()\n",
    "\n",
    "    for res in results:\n",
    "        label = res['label']\n",
    "        score = res['score']\n",
    "        label_count[label] += 1\n",
    "        score_sum[label] += score\n",
    "\n",
    "    # 각 레이블의 평균 스코어 계산\n",
    "    average_scores = {label: (score_sum[label] / label_count[label]) for label in label_count}\n",
    "\n",
    "    print(\"Sentiment analysis completed.\")\n",
    "\n",
    "    return label_count, average_scores\n",
    "\n",
    "# 전체 파이프라인 함수\n",
    "def sentiment_analysis_pipeline(folder_path):\n",
    "    print(\"Starting sentiment analysis pipeline...\")\n",
    "\n",
    "    # PDF에서 텍스트 추출\n",
    "    text = extract_text_from_pdfs(folder_path)\n",
    "\n",
    "    # 텍스트 전처리\n",
    "    cleaned_text = preprocess_text(text)\n",
    "\n",
    "    # 감정 분석 수행\n",
    "    label_count, average_scores = analyze_sentiment_text(cleaned_text)\n",
    "\n",
    "    print(\"Sentiment analysis pipeline completed.\")\n",
    "\n",
    "    return label_count, average_scores\n",
    "\n",
    "# 전체 파이프라인 실행\n",
    "label_count, average_scores = sentiment_analysis_pipeline(pdf_folder)\n",
    "\n",
    "# 긍정, 부정 결과 집계 및 출력\n",
    "positive_count = label_count.get('POSITIVE', 0)\n",
    "negative_count = label_count.get('NEGATIVE', 0)\n",
    "positive_score = average_scores.get('POSITIVE', 0)\n",
    "negative_score = average_scores.get('NEGATIVE', 0)\n",
    "\n",
    "print(f\"Positive: {positive_count}, Average Score: {positive_score:.2f}\")\n",
    "print(f\"Negative: {negative_count}, Average Score: {negative_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4a8b0-8da8-4a42-8b3c-309e2b48221d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
